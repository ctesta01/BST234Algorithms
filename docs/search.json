[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "",
    "text": "Building 2, room 435\nResearch interests: algorithmic mathematics and statistics, efficient and randomized algorithms, methodology development, multiple testing, NP-complete problems.\nTA: Max Wang\nTake home midterm and final presentation project.\nWeek after of Spring Break is midterm."
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1: Intro",
    "section": "",
    "text": "An iconic algorithm:\nThis algorithm is the greatest common divisor.\nThis is used in encryption and it is still the fastest algorithm.\nFrom ~300 BC from Euclid of Alexandria.\nExamples of algorithms:\nExamples:\nGeorg’s hit-list:\nFactoring numbers is super-polynomial but sub-exponential. It’s like almost exponential. This doesn’t go for factoring polynomials.\nWhen multiplying numbers, all the terms get jumbled up whereas we know where the pieces came from in polynomials:\nProperties of Algorithms\nAlgorithms work on dynamic sets of elements (input -&gt; output):\nData structures are used for the implementation of dynamic datasets. The efficiency of a data structure can depend on the desired manipulation operation.\nExamples of data structures:\nCompare and contrast the array, doubly-linked list, and the heap:\nThe array has the property that if every element is of the same structure and takes up the same amount of memory, we can access the nth element quickly by calculating where it will be in memory and looking there. However, to update an array can be annoying because if it is growing and the neighboring memory is not free, we might have to move the whole array.\nThe doubly-linked list facilitates things like:\nBut search algorithms are limited to stepping through the list until a desired element is found.\nOn the other hand, the heap allows for intelligent searching. Of course heaps aren’t good for everything: for example, accessing the nth element would be easier in an array than in a heap.\nReferences:\nBerlekamp’s algorithm https://en.wikipedia.org/wiki/Berlekamp%27s_algorithm\nRotating doubly-linked list: https://tex.stackexchange.com/questions/242044/add-label-to-a-rotated-doubly-linked-list",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Intro</span>"
    ]
  },
  {
    "objectID": "index.html#goals-for-the-course",
    "href": "index.html#goals-for-the-course",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "Goals for the course",
    "text": "Goals for the course\nIntroduction to important computational problems in computer science (biostatistics) and state of the art algorithms for solving them.\nUnderstanding of data structures and algorithms to solve problems of practical relevance.\nUnderstanding of mathematical standards for numerical analysis and statistics, inclusding their implementations.\nKnowledge of R and Python is vital (lab sessions).\nWhy is it always “Data Structures And Algorithms”?\nBecause the two are intrinsically linked; they don’t mean much without the other.\nWhy? Because the algorithm may run much faster or slower depending on the data structure.\nFor example, we might have a list. Or we might have a hash-map. Some things are fast in lists, whereas different things are fast in a hash-map.\nOften we have an end-pointer if we have a doubly-linked list.\nFor example, popping the first element, or last element, these are \\(O(1)\\) operations (including updating pointers). In contrast, searching is an \\(O(n)\\) operation when traversing.\nIf we have a binary tree (heap), then searching is \\(O(\\log_2 n )\\).\nWe will use the Cormen, Leiserson, Rivest, Stein. CLLS. Available for free on HOLLIS.\nContents\n\nIntroduction, random numbers\nConcepts of algorithms, complexity and sorting algorithms\nData structures and heapsort\nGreedy algorithms and dynamic programming\nIntroduction to parallel programming\nP and NP\nNumerical aspects of computer algorithms, condition of a problem, numerical stability of an algorithm, numerical error, forward/backward-error\nEfficient algorithms for linear algebra\nLeast-Squares Program, Eigenvalue Decomposition\nNumerical integration, Monte Carlo integration, importance sampling\nNumerical optimization\nWork on projects\nProject presentations"
  },
  {
    "objectID": "week1/week1.html#section",
    "href": "week1/week1.html#section",
    "title": "Week 1: Intro",
    "section": "",
    "text": "function gcd(a,b): \n  while a ≠ b:\n    if a &gt; b:\n      a := a-b\n    else: \n      b := b-a\n  return a\n\ngcd: 20, 15 -&gt; 5 \n\na = 20, b = 15 \na = 5, b = 15, \na = 5, b = 1 \na = 5, b = 5\n\n\n\n\n\n\nSorting Algorithms\nOptimization Problems:\n\nTraveling salesman: chip design, airline schedule\n\nMathematical problems:\n\nSolution of linear equations / matrix algebra\nIntegration\n\nStatistical problems\n\nOptimal designs / power calculations\nComputation of distributions\nComputation of test statistics\n\n\n\n\nEuclid GCD\nPagerank\nGradient descent\n\nLocal minimum\n\nBellman; Ford; shortest path and allow negative weights\nDjikstra\nCompression\nWord2Vec\nShannon Encoding for Transmission Errors\nTower of Hanoi\n\nRecursive\n\n\n\n\nEuclid\nQuadratic Sieve\nBerlekamp\nCantor-Zassehaus\n\n\n\n(123 * 827) = (1*10^2 + 2 * 10^2 + 3 * 10^0 ) * (...)\n\nvs. \n\n(2x + 3)(x^2 + 1)\n\nWhat do random numbers and compression have in common?\n\n\n\nFeasibility\nTermination\nDeterministic\nFinite\n\n\n\nSearching, inserting and deleting elements\nMaximum, minimum element\n\n\n\n\nArray: access to elements based on index\n\nContinuous allocated memory, evenly divided\n\nLinked List: reference / pointer to the next element\nStack: dynamic set of elements, last in first out\nQueue: dynamic set of elements, can only be read starting from most longest added element, first-in-first-out\nGraphs or Trees: elements have references/pointers to a variable number of other elements\n\n\n\n\n\n\n\n\n\n\n\n\nEasy insertions or deletions anywhere\nQuickly accessing the first and last elements\nEasy storage in memory since memory doesn’t need to be in one contiguous block",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Intro</span>"
    ]
  },
  {
    "objectID": "week1/week1.html#day-2",
    "href": "week1/week1.html#day-2",
    "title": "Week 1: Intro",
    "section": "Day 2",
    "text": "Day 2\nWe’ve moved classrooms to Kresge G1. On 7th of February, and 4th of March we need to be in FXB G12.\nLast time we talked about algorithms like sorting, searching, talking about their time / space complexity.\nAlgorithms need to have finite numbers of instructions.\nDeterministic means there is no randomness.\nWell-definedness means that the computer knows what to do at every step.\nProperties of algorithms include:\n\nFeasibility\nTermination\nDeterministic\n\nThe output is well-defined for every instance\nAt any time, the next step is well-defined\n\nFinite\n\nThe number of steps must be finite\nAt any time point, the required memory must be finite\n\n\nToday let’s discuss about random numbers.\nMotivation could include:\n\nStatistical simulation (Monte Carlo) in statistical methods research.\nThe statistical theories/methods are all based on assumptions. So most theorems state assumptions…\nThe theories can hardly be verified in real world data because\n\n\nthe real data never satisfy the assumption; and (2) the underlying truth is unknown (gold standard)\n\n\nIn simulation, data are created in a well controlled environment (model assumptions) and all truth are known. So the claim in the theorem can be validated and subjected to sensitivity analysis.\n\nWe often use the inverse CDF method to generate random numbers that are distributed a certain way. Or importance sampling. Or metropolis-hastings.\nBut how do we generate uniform random numbers? runif(10).\n\nCould it be just a list somewhere just stored\nIt could be some highly erratic but still deterministic function.\n\nWhy are ZIP CRC Checksums related to randomness? How do those relate to random numbers?\nWhatever we use to produce random numbers, we can perform statistical tests on the output to see if it’s distinguishable from randomness.\nSome good properties of a random number generator:\n\nUniformity\nIndependence\nDiehard tests (They should pass “Diehard” tests, qc-tests for PRNG)\nReplication\nCycle length - should be a long time before numbers repeat\nSpeed (fast)\nMemory usage (little need)\nParallel implementation (desirable for speed)\nCryptographically secure: required for password storage\n\nThe last point refers to the idea that there should be no ‘reversal’; it should not be possible to look at the random numbers generated and tell what the seed is.\nIf we have \\(f: s \\to [0,1]\\), we want to check two things:\n\nIs the output erratic? (Crucial)\nDoes $f^{-1} exist? (Can it be hacked)? (Optional)\n\n\nMid-square method\n\nStart with a 4-digit number \\(z_0\\), then square it. Should be an 8 digit number, and if not pad it.\nDeterministic sequence of numbers $e.g., \\(z_{n+1} = f(z_n)\\). Look at the middle four digits of it. Put the decimal in front.\nRepeat\n\nWe get uniform random number.\nBut this has problems, like two successive zeroes behind the decimal will never disappear.\n\n\nLinear Congruential Generator\nLCG is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. They produce a sequence of integers between 0 and m-1 according to \\(z_n = (a * z_{n-1} + c) \\pmod m, \\quad n = 1, 2, ...\\)\nwhere \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) the modulus. To obtain uniform random numbers on \\((0,1)\\), we take\n\\[u_n = z_n / m.\\]\nGood choices for \\(a\\), \\(c\\), and \\(m\\) are important.\nMarsaglia showed that \\((z_i, z_{i+1})\\) have this non-random structure.\nGood question: Is there a way to determine the number of hyperplanes?\nThe theory behind LCG is relatively easy to understand, and are easily implemented and fast, especially on computer hardware which can provide modulo arithmetic by stoage-bit truncation.\nA linear congruential generator has full period (=cycle length m) if and only if\n\nThe only positive integer that divides both \\(m\\) and \\(c\\) is 1 (the gcd)\nIf \\(q\\) is a prime number that divides \\(m\\), then \\(q\\) divides \\(a-1\\);\nIf 4 divides \\(m\\), then 4 divides \\(a-1\\).\n\nMarin Mersenne\nFor each Mersenne number, it’s prime if and only if it does not divide the corresponding shadow sequence number: \\(S_2 = 4\\), \\(S_n = S_{n-1}^2 - 2\\).\nimport numpy as np\nimport hashlib\nimport matplotlib.pyplot as plt\n\ndef hashing(s):\n    h = hashlib.md5()\n    #h = hashlib.sha256()\n    h.update(str(s).encode(\"utf-8\"))\n    return(h.hexdigest())\n\ndef random_number(seed):\n    temp = hashing(str(seed))\n    x = int(temp,16) / (16**len(temp))\n    return(x)\n\nif __name__==\"__main__\":\n    print(hashing(\"mouse\"))\n    print(hashing(\"house\"))\n    print(random_number(5))\n    # plot n numbers\n    n = 1000\n    x = np.arange(n)\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = random_number(x[i])\n    plt.plot(x,y,\"+\",color=\"black\")\n    plt.show()\nJohn the Ripper tries to invert the MD5 hashsum.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Intro</span>"
    ]
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Concepts of Algorithms\nContents:\nAn instance of is a particular implementation of an algorithm. It should satisfy all the same properties as the original algorithm.\nThings to keep in mind:\nLet’s consider the runtime of insertion sort:\nIn step \\(i\\), we look at \\(n-i\\) values to compare.\n\\[\n\\sum_{i=1}^{n-1} n-i = \\sum_{i=1}^{n-1} i = \\frac{n(n-1)}{2} \\sim n^2\n\\]\n\\[\n\\sum_{i=1}^n i^2 \\sim n^3\n\\]\nIn general, is it always one more?\nYes, because we perform \\(n\\) operations of \\(i^{...}\\) complexity.\nWe can use comparison of an integral to a summation\n\\[\n\\sum_{i=1}^\\infty i^2 \\leq \\int_{i=1}^{\\infty} i^2\n\\]\nInsertion Sort(A)\nOne way of proving correctness of a sequential algorithm consists in finding an invariant which is always true in each iteration independently of the progress of the algorithm.\nHere: What is the invariant statement for insertion sort? The invariant is that in iteration i, in the array A[1..i-1] are sorted.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week2/week2.html#concepts-of-algorithms",
    "href": "week2/week2.html#concepts-of-algorithms",
    "title": "Week 2",
    "section": "",
    "text": "Concepts of algorithms: what is an algorithm? What flavors of algorithms exist?\n\n\nComplexity in time and space\nRecursion\nDivide-and-conquer paradigm\nExample: Sorting algorithms\nExample: Traditional and fast multiplication via divide-and-conquer.\n\n\n\n\n\n\n\nNote\n\n\n\nWhat is an algorithm?\nA finite list of instructions that is well-defined. We can think of it as a function from input to output space: \\(f: S \\to T\\).\nWhat properties do they have?\n\nRuntime: They can terminate after finite time, or have finite expected runtime.\nComplexity in time & space.\nAlgorithms can be iterative, or recursive.\n\n\n\n\n\n\nEvery step must be feasible.\nThe algorithm terminates after a finite number of steps.\nDeterminisms\n\n\n\n\n\n\n\nTip\n\n\n\nThere are two types of randomness: those that go into results, or those that don’t.\nTwo examples:\n\nDrawing random numbers; MCMC\nSorting numbers (random runtime)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| eval: false\nfor j &lt;- 2 to length[A]\n  do key &lt;- A[j]\n  &gt; Insert A[j] into the sorted sequence A[1..j-1].\n  i &lt;- j-1\n  while i &gt; 0 and A[i] &gt; key\n   do A[i+1]\n     i &lt;- i - 1\n   A[i+1] &lt;- key\n\n\n\\begin{algorithm} \\caption{Insertion-Sort} \\begin{algorithmic} \\Procedure{InsertionSort}{$A$} \\For{$j = p$ \\To $r - 1$} \\State $y \\gets A[j]$ \\State $\\triangleright$ Insert $A[j]$ into the sorted sequence $A[1..j-1]$ \\State $i \\gets j-1$ \\EndFor \\While{$i &gt; 0$ and $A[i] &gt; key$} \\State $A[i+1] \\gets A[i]$ \\State $i \\gets i - 1$ \\EndWhile \\State $A[i + 1] \\gets key$ \\EndProcedure \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week2/week2.html#growth-of-functions-and-mathcal-o-notation",
    "href": "week2/week2.html#growth-of-functions-and-mathcal-o-notation",
    "title": "Week 2",
    "section": "Growth of Functions and \\(\\mathcal O\\)-notation",
    "text": "Growth of Functions and \\(\\mathcal O\\)-notation\nWe define for functions \\(f(n)\\) and \\(g(n)\\):\n\nAsymptotically tight bound: \\[\n    \\begin{align*}\n    \\mathcal O(g(n)) & = \\{ f(n) :  \\text{ there exist constants } c_1, c_2 &gt;\n    0 \\\\\n    & \\text{ and } n_0 \\text{ such that } 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\forall n \\geq n_0 \\}\n    \\end{align*}\n    \\]\nLitte-o notation (convergence to zero):\n\n\\[\\mathcal o(g(n)) = \\left\\{ f(n) \\colon \\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = 0\\right\\}\\]\n\n“Big-O” notation (asymptotic upper bound): \\[\n  \\mathcal O(g(n)) = \\left\\{\n    \\begin{align*}\n    & f(n) : \\text{there exists constants } c &gt; 0 \\text{ and } \\\\\n    & n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n)\n    \\forall n \\geq n_0\n    \\end{align*}\n    \\right\\}\n  \\]\n\n\n\n\\begin{algorithm} \\caption{If-Example} \\begin{algorithmic} \\Procedure{If-Example}{$x$} \\If{$x &lt; 100$} \\State $y \\gets x$ \\Else \\For{$i \\gets 1$ \\To $n$} \\If{$A[i] &gt; y$} \\State $y \\gets A[i]$ \\EndIf \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nUpper-triangulization (Gaussian elimination) is \\(\\mathcal O(n^3)\\).\nFor calculating the determinant, we can use either:\n\nLaplacian expansion: (use minors)\n\nWhich gives us \\(\\mathcal O(n!) \\subset \\mathcal O(e^n)\\).\n\nDo Gaussian elimination and then multiply the diagonal: \\(\\mathcal O(n^3)\\). So this is \\(\\mathcal O(n^3) + \\mathcal O(n)\\), which is just \\(\\mathcal O(n^3)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week2/week2.html#wednesday",
    "href": "week2/week2.html#wednesday",
    "title": "Week 2",
    "section": "Wednesday",
    "text": "Wednesday\nP Set principles: Practice \\(\\mathcal O\\) notation, mathematical relations, induction, and a simple algorithm to analyze.\nLast time, we did insertion sort.\n\nRecursion\nA recursive algorithm is an algorithm which calls itself with ”smaller (or simpler)” input values.\nA sequential algorithm example:\n\n\n\n\n\\begin{algorithm} \\caption{Sequential-Example} \\begin{algorithmic} \\Procedure{Sequential-Example}{} \\State{res $\\gets$ 1} \\For{$i = 1$ \\To $n$} \\State res = res $\\cdot \\; i$ \\EndFor \\State \\textbf{return} res; \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Factorial} \\begin{algorithmic} \\Procedure{factorial}{n} \\If{$n = 1$} \\Return 1 \\Else \\Return {$n \\; \\cdot \\;$ \\textsc{factorial} $(n-1)$} \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\nAn example of a problem not amenable to divide-and-conquer is the traveling salesman problem. If we calculate the optimal route over half the problem, it’s not known how to combine sub-solutions.\nhttps://webgol.dinfo.unifi.it/OptimizationModels/TravelingSalesPerson.html\nTo use divide-and-conquer we have to be in a situation where we can\n\nDivide\nConquer\nCombine\n\n\n\nMerge-Sort\nThe crux is that we can combine two sorted lists in linear time.\nJust combining is \\(\\mathcal O(n)\\).\n\n\n\\begin{algorithm} \\caption{Merge} \\begin{algorithmic} \\Procedure{Merge}{$a_1, a_2$} \\State $i \\gets 1$ \\State $j \\gets 1$ \\State $k \\gets 1$ \\State $c \\gets $ \\textsc{Array}($n$) \\While{$i,j &lt; n$} \\If{$a_{1i} \\leq a_{2j}$} \\State $c_k \\gets a_{1i}$ \\State $i \\gets i +1$ \\Else \\State $c_k \\gets a_{2j}$ \\State $j \\gets j + 1$ \\EndIf \\State $k \\gets k + 1$ \\EndWhile \\Return $c$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\\begin{algorithm} \\caption{MergeSort} \\begin{algorithmic} \\Procedure{MergeSort}{$a$} \\State $A_1 \\gets$ \\textsc{HalfOf}$(A)$ \\State $A_2 \\gets$ \\textsc{TheOtherHalf}$(A)$ \\State $S_1 \\gets$ \\textsc{MergeSort}$(A_1)$ \\State $S_2 \\gets$ \\textsc{MergeSort}$(A_2)$ \\Return \\textsc{Merge}$(S_1, S_2)$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\\usetikzlibrary{decorations, calc, arrows, arrows.meta, positioning}\n\n\\begin{tikzpicture}[\n  scale=4,\n  array/.style={rectangle, draw, inner sep=5pt, text=black, minimum width = 20pt, minimum height = 20pt}\n  ]\n\n  % Nodes --- \n  \\node[array] (n11) {38};\n  \\node[array, right=0cm of n11] (n12) {27};\n  \\node[array, right=0cm of n12] (n13) {43};\n  \\node[array, right=0cm of n13] (n14) {3};\n  \\node[array, right=0cm of n14] (n15) {9};\n  \\node[array, right=0cm of n15] (n16) {82};\n  \\node[array, right=0cm of n16] (n17) {10};\n\n  \\node[array, below left=.5cm of n11] (n21) {38};\n  \\node[array, right=0cm of n21] (n22) {27};\n  \\node[array, right=0cm of n22] (n23) {43};\n  \\node[array, right=0cm of n23] (n24) {3};\n\n  \\node[array, right=2cm of n24] (n25) {9};\n  \\node[array, right=0cm of n25] (n26) {82};\n  \\node[array, right=0cm of n26] (n27) {10};\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\n1. Divide A into A_1, A_2\n2. S_1 = MergeSort(A_1)\n   S_2 = MergeSort(A_2)\n3. S = MergeSort(S_1, S_2)\nThe complexity of \\(\\texttt{MergeSort}\\) on \\(n\\) variables is \\(\\mathcal O(2 T(\\frac{n}{2}) + n)\\) where \\(T(n/2)\\) is the time complexity of \\(\\texttt{MergeSort}\\) on \\(n/2\\) variables.\nSo \\(\\mathcal O(\\texttt{MergeSort}) = 2 [ 2 T(n/4) + n/2] + n\\)…\n\\[ = 2^2 T(n / 2^2) + 2n\\]\n\\[ ... \\]\n\\[ = 2^{\\log_2{n}} \\cdot T(1) + \\log_2(n) n\\]\n\\[ T(n) = n + n \\cdot \\log_2(n) \\in \\mathcal O(n \\log_2(n))\\]\nFor divide-and-conquer problems, we have time complexity given by\n\nhow many sub-problems we divide into\nsize of sub-problems\neffort to marshall the sub-problems (combine)\n\n\\[T(n) = a \\cdot T(\\frac{n}{b}) + n^c.\\]\nNow we prefer to have \\(a\\) small, \\(b\\) big, and \\(c\\) small.\nThe Master theorem yields asymptotically tight bounds to recurrences from divide and conquer algorithms that partition an input into smaller subproblems of equal sizes.\nThe crucial element is thus: what is the (log) ratio of the number of new subproblems to the size of new subproblems. I.e., \\(\\log_b(a)\\), the critical exponent.\nTheorem (Master Theorem). Let the runtime of an algorithm be given as\n\\[T(n) = a T(\\frac{b}{n}) + f(n)\\]\nand let the critical exponent be\n\\[c_{\\text{crit}} = \\log_b(a) = \\log(\\text{n of subproblems})/\\log(\\text{subproblem size}).\\]\nThen we have three cases:\n\nIf \\(f(n) = \\mathcal O(n^c)\\) for some \\(c &lt; c_{\\text{crit}}\\) then \\(T(n) = \\Theta(n^{c_{\\text{crit}}})\\).\nIf \\(f(n) = \\Theta(n^{c_{\\text{crit}}} \\log^k n)\\) then \\(T(n) = \\Theta(n^{c_{\\text{crit}}} \\log^{k+1} n).\\)\nIf \\(f(n) = \\Omega(n^c)\\) for some \\(c &gt; c_{\\text{crit}}\\) then no definite statement can be made yet. However, if \\(a \\cdot f(n/b) \\leq\nk \\cdot f(n)\\) for some \\(k &lt; 1\\) and large enough \\(n\\) then \\(T(n) = \\Theta(f(n)).\\)\n\n1 corresponds to work splitting and recombining a problem being dominated by work on subproblems.\n2 corresponds to work to split/recombin a problem being comparable to work on subproblems.\n3 is whre work to split/recombine a problem dominates subproblems.\nThe runtime of MergeSort can be expressed as \\[T(n) = 2 T\\left(\\frac{n}{2}\\right) + n\\]\nTherefore \\(a = b = 2\\) and \\(f(n) = n\\).\nThe critical exponent is \\(c_{\\text{crit}} = \\log_b(a) = 1\\).\nCase 2 applies since \\(f(n) = \\Omega(n)\\) with \\(k = 0\\).\nTherefore the runtime is\n\\[T(n) = \\Omega(n^{c_{\\text{crit}}} \\log^{k+1}n) = \\Theta (n \\log n).\\]\n\n\n\n\n\n\nWarning\n\n\n\nRemember that \\[n \\log_3(n) = \\frac{\\log_2(n)}{\\log_2(3)},\\] so it doesn’t matter which base of the \\(\\log\\) we use in \\(\\mathcal O\\) notation.\n\n\n\n\nMultiplication\nInstead of writing 3456 * 8957 in our standard long-form multiplication, we can use divide-and-conquer:\n\\[=(34*89*10^4) + (34*57*10^2) + (56*89*10^2) + 56*57\\]\n\\[T(n) = 4 \\cdot T(\\frac{n}{2}) + n\\]\n3456 * 8957\n34|56 * 89|57\n\n= (34 * 10^2 + 56) * (89 * 10^2 + 57)\n\n(34 + 56)(89 + 57)\n= 34*89 + 34*57 + 56*89 + 56*57\n\nT(n) = 3 T(n/2) + n\nc_{\\text{crit}} = \\log_2(3) = 1.6\nO(n^{1.6})",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "import numpy as np \n\nx = [1,2,3,4]\n\ndef merge(x,y):\n  i = 0\n  j = 0\n  z = []\n  while (i &lt; len(x) and j &lt; len(y)):\n    if (i == len(x)):\n      z.append(y[j])\n      j = j + 1\n      print(z)\n    elif (j == len(y)):\n      z.append(x[i])\n      i = i + 1 \n    elif (x[i] &lt; y[j]):\n      z.append(x[i])\n      i = i + 1\n    else:\n      z.append(y[j])\n      j = j + 1\n  return z\ndef merge2(x, y):\n  px = 0\n  py = 0\n  z = np.zeros(x.size + y.size, dtype=int)\n\n  for i in range(len(z)):\n    if px&lt;len(x) and py&lt;len(y):\n      if x[px] &lt;= y[py]:\n        z[i] = x[px]\n        px = px+1\n      else:\n        z[i] = y[py]\n        py = py+1 \n    elif px==len(x):\n      z[i] = y[py]\n      py = py+1\n    elif py==len(y):\n      z[i] = x[px]\n      px = px+1\n  return(z)\n\ndef mergesort(x):\n  if len(x) == 1:\n    return(x)\n  else:\n    mid = len(x)//2\n    s1 = mergesort(x[:mid])\n    s2 = mergesort(x[mid:])\n    return(merge2(s1, s2))\n\nn = 10\nx = np.random.randint(100,size=n)      \ny = np.random.randint(100,size=n)      \nx = np.sort(x)\ny = np.sort(y)\nmerge2(x,y)\n\nmergesort(x)\nExample cpp code for merge sort\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\nusing namespace std;\n\nvector&lt;int&gt; merge(vector&lt;int&gt; x, vector&lt;int&gt; y){\n  int i = 0;\n  int j = 0;\n  vector&lt;int&gt; z;\n  while (i &lt; x.size() && j &lt; y.size()){\n    if (x[i] &lt; y[j]){\n      z.push_back(x[i]);\n      i++;\n    }\n    else{\n      z.push_back(y[j]);\n      j++;\n    }\n  }\n  while (i &lt; x.size()){\n    z.push_back(x[i]);\n    i++;\n  }\n  while (j &lt; y.size()){\n    z.push_back(y[j]);\n    j++;\n  }\n  return z;\n}\nDemo of how to run code in Rcpp\n\nlibrary(Rcpp)\ncppFunction('\n  std::vector&lt;int&gt; merge(std::vector&lt;int&gt; x, std::vector&lt;int&gt; y){\n    int i = 0;\n    int j = 0;\n    std::vector&lt;int&gt; z;\n    while (i &lt; x.size() && j &lt; y.size()){\n      if (x[i] &lt; y[j]){\n        z.push_back(x[i]);\n        i++;\n      }\n      else{\n        z.push_back(y[j]);\n        j++;\n      }\n    }\n    while (i &lt; x.size()){\n      z.push_back(x[i]);\n      i++;\n    }\n    while (j &lt; y.size()){\n      z.push_back(y[j]);\n      j++;\n    }\n    return z;\n  }\n')\nx &lt;- c(1,2,3,4)\ny &lt;- c(2,3,4,5)\nmerge(x,y)\n\n[1] 1 2 2 3 3 4 4 5\n\n\nExamples of the data structures we’re interested in include:\n\nArray\nLinked List\nStack (LIFO)\nQueue (FIFO)\nGraphs or Trees\n\n// struct link\n// {\n//   link* next;\n//   int data;\n// };\n\nstruct link\n{\n  link* next;\n  int data;\n  link(int a_data, link* a_next = 0): next(a_next), data(a_data) {}\n};\n\nlink* small_primes = new link(2, new link(3, new link(5, new link(7))));\n\n// or \n// \ntemplate&lt;typename T&gt; struct link\n{\n  link* next;\n  T data;\n  link(T a_data, link* a_next = 0): next(a_next), data(a_data) {}\n};\nimport numpy as np \n\nclass Node: \n  def __init__(self, data=None): \n    self.value = data\n    self.next = None\n\nx = np.random.randint(100,size=10)\nprint(x)\n\nstart = Node(data = x[0])\n\nprev = start\nfor i in range(1, x.size):\n  temp = Node(x[i])\n  prev.next = temp\n  prev = temp\n\nprint(start.value)\nprint(start.next.value)\n\nwhile True:\n  print(start.value)\n  if start.next == None:\n    break\n  start = start.next\nimport numpy as np \n\nclass Node: \n  def __init__(self, data=None): \n    self.value = data\n    self.left = None\n    self.right = None\n\ndef valsToTree(l):\n  newnode = Node()\n  newnode.value = l[0]\n  l = l[1:]\n  if len(l) == 1:\n    newnode.left = Node(l)\n  elif len(l) &gt; 1:\n    mid = len(l)//2\n    newnode.left = valsToTree(l[:mid])\n    newnode.right = valsToTree(l[mid:])\n  return newnode",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week2/week2.html#at-home",
    "href": "week2/week2.html#at-home",
    "title": "Week 2",
    "section": "At Home",
    "text": "At Home\n\nhttps://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)\nhttps://en.wikipedia.org/wiki/Big_O_notation\nhttps://en.wikipedia.org/wiki/Multiplication_algorithm#Long_multiplication\nhttps://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture3.pdf\nhttps://www.geeksforgeeks.org/advanced-master-theorem-for-divide-and-conquer-recurrences/\nhttps://www.tutorialspoint.com/design_and_analysis_of_algorithms/design_and_analysis_of_algorithms_masters_theorem.htm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Recapping P-set\n3b: \\[\n\\begin{aligned}\nT_2(n) & = 25 T_2(n/5) + 13n^3 \\\\\n& = 25^{\\log_5 n}T_2(1) + \\sum_{i=0}^{\\log_5(n)} 25^i \\cdot 13 \\cdot (n/5^i)^2\n\\end{aligned}\n\\]\nSimplify the last expression. Should get \\(\\sim n^2 \\log n\\).\n\\usetikzlibrary{shapes} \n\\begin{tikzpicture}[\n    scale=2, \n    every node/.style={circle, draw=black}]\n  \\node {1}\n    child {node {2}\n      child {node {4}}\n      child {node {5}}\n    }\n    child {node {3}\n      child {node {6}}\n      child {node {7}\n        child {node {8}}\n        child {node {9}}\n        }\n    };\n\\end{tikzpicture}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week4/week4.html#heaps---day-2",
    "href": "week4/week4.html#heaps---day-2",
    "title": "Week 4",
    "section": "Heaps - Day 2",
    "text": "Heaps - Day 2\n\n\\usetikzlibrary{shapes, trees} \n\n\\tikzset{\n  heap/.style={\n    every node/.style={circle,draw},\n    level 1/.style={sibling distance=30mm},\n    level 2/.style={sibling distance=15mm}\n    level 3/.style={sibling distance=5mm}\n  }\n}\n\n\\begin{tikzpicture}[\n    scale=2, \n    heap\n    ]\n\n  \\node {16}\n    child {node {14}\n      child {node {8}\n        child {node {2}}\n        child {node {4}}\n      }\n      child {node {7} \n        child {node {1}}\n      }\n    }\n    child [sibling angle=10] {node {10}\n      child {node {9}}\n      child {node {3}}\n    };\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\nComparison of heapsort and quicksort:\n\nHeapsort works in place\nA heap is a complete tree, where every node’s children must be greater than their parents.\nHeapsort: \\(O(n \\log n)\\)\n\nIn heapsort, we know where our biggest element is: at the top. So we prune it, and we can re-establish the heap property in log(n) time-complexity. Recall that to update, we pick the last element in the heap (bottom row, right-most), and we can maxHeapify once more.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week5/week5.html",
    "href": "week5/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "Review of topics:\n\nKaratsuba’s Trick\nMerge Sort\nHeap Sort\nQuick Sort\nPrim’s Algorithm\nKruskal’s algorithm\nFloyd’s Cycle Detection Algorithm\nGreedy Algorithms\nDynamic Programming\nDijkstra’s Algorithm 2\nBellman-Ford Algorithm, 2\nActivity Selection\n\nOptimal Substructure\n\n\n\nKaratsuba’s Trick\n\n# R code to implement Karatsuba's trick\nkaratsuba &lt;- function(x, y) {\n  if (x &lt; 10 || y &lt; 10) {\n    return(x * y)\n  }\n  \n  n &lt;- max(nchar(x), nchar(y))\n  m &lt;- n %/% 2\n  \n  a &lt;- x %/% 10^m\n  b &lt;- x %% 10^m\n  c &lt;- y %/% 10^m\n  d &lt;- y %% 10^m\n  \n  ac &lt;- karatsuba(a, c)\n  bd &lt;- karatsuba(b, d)\n  ad_bc &lt;- karatsuba(a + b, c + d) - ac - bd\n  \n  return(ac * 10^(2 * m) + ad_bc * 10^m + bd)\n}\n\n\n\nKruskal’s Algorithm\n\n# R code to implement Kruskal's algorithm\n\n# set up the data\nvertices &lt;- c('a', 'b', 'c', 'd', 'e', 'f', 'g')\n\nedges &lt;- list(\n  c('a', 'b', 4),\n  c('a', 'c', 3),\n  c('b', 'c', 2),\n  c('b', 'd', 3),\n  c('c', 'd', 5),\n  c('c', 'e', 4),\n  c('d', 'e', 1),\n  c('d', 'f', 6),\n  c('e', 'f', 7),\n  c('e', 'g', 5),\n  c('f', 'g', 8)\n)\n\nedges_df &lt;- data.frame(do.call(rbind, edges))\ncolnames(edges_df) &lt;- c('v1', 'v2', 'weight')\nedges_df$weight &lt;- as.numeric(as.character(edges_df$weight))\n\n# sort the edges\nedges_df &lt;- edges_df[order(edges_df$weight),]\n\n# initialize the forest\nforest &lt;- data.frame(v1 = c(), v2 = c(), weight = c())\n\n# function to check for cycles\ncheck_if_introduces_cycle &lt;- function(forest, new_edge) {\n  if (nrow(forest) == 0) {\n    return(FALSE)\n  }\n  # g &lt;- graph_from_data_frame(rbind(forest, new_edge), directed = FALSE)\n\n  unionfind \n  # return(has_eulerian_path(g))\n}\n\n# loop through candidates for edges to construct MST\nfor (i in 1:nrow(edges_df)) {\n  if (\n    (! edges_df[[i,'v1']] %in% c(forest$v1, forest$v2) ||\n    ! edges_df[[i,'v2']] %in% c(forest$v1, forest$v2)) && \n    # check for cycles \n    ! check_if_introduces_cycle(forest, edges_df[i,])\n    ) {\n    forest &lt;- rbind(forest, edges_df[i,])\n  }\n}\n\nforest\n\n# visualize \nlibrary(igraph)\ng &lt;- graph_from_data_frame(edges_df, directed = FALSE)\nl &lt;- layout.reingold.tilford(g)\nplot(g, edge.label = edges_df$weight, layout = l)\n\nf &lt;- graph_from_data_frame(forest, directed = FALSE)\nplot(f, edge.label = forest$weight, layout = l)\n\n\n\nActivity Selection Problem\nSort the activities by finish time. \nSchedule the first activity in that ordering.\nThen schedule the next activity which starts after the \nprevious activity finishes. \nRepeat until no more candidate activity exists. \n\n\nOptimal Substructure\nThis forms a recipe for greedy methods:\nFormulate a problem such that we have to make a choice and are left with one subproblem afterwards.\nProve there is always an optimal solution that uses the greedy choice.\nShow that combining the greedy choice and th eoptimal subproblem solution yields an optimal solution.\nSolve top-down with the greedy choice. One might have to process the input to make it suitable for the greedy choice, like sorting the input.\n\n# example of setting up and solving an activity selection problem in R code\n\n# set up the data\nstart &lt;- c(1, 3, 0, 5, 8, 5)\nfinish &lt;- c(2, 4, 6, 7, 9, 9)\n\n# sort the data\nactivity &lt;- data.frame(start, finish)\nactivity &lt;- activity[order(activity$finish),]\n\n# greedy choices\nchoices &lt;- c(1)\n\n# repeat\nfor (i in 2:nrow(activity)) {\n  if (activity$start[i] &gt;= activity$finish[choices[length(choices)]]) {\n    choices &lt;- c(choices, i)\n  }\n}\n\nactivity[choices,]\n\n  start finish\n1     1      2\n2     3      4\n4     5      7\n5     8      9",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week6/week6.html",
    "href": "week6/week6.html",
    "title": "Week 6",
    "section": "",
    "text": "Djikstra’s Algorithm\nIf \\(Z\\) is a vertex on the shortest path \\(p\\) from A to F then the optimal path from A to Z and from Z to F must be contained in \\(p\\).\nDynamic programming formulation: \\[d(x,y) = \\min_{z \\in V} \\{ d(x, z) + d(z, y)\\},\\] where \\(d(\\cdot, \\cdot)\\) is the shortest distance between two nodes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week6/week6.html#parallel-programming",
    "href": "week6/week6.html#parallel-programming",
    "title": "Week 6",
    "section": "Parallel Programming",
    "text": "Parallel Programming\n\nEasy examples with cluster computing, slurm jobs…\nI am reminded of http://srmcse.weebly.com/uploads/8/9/0/9/8909020/introduction_to_parallel_computing_second_edition-ananth_grama..pdf\n\nThreading in Python\n\nhttps://realpython.com/intro-to-python-threading/\nhttps://www.geeksforgeeks.org/multithreading-python-set-1/\nhttps://stackoverflow.com/questions/31340/how-do-threads-work-in-python-and-what-are-common-python-threading-specific-pit\n\nDistributed Computing over Donated Compute Time:\n\nhttps://www.mersenne.org/\nhttps://www.pennmedicine.org/news/news-blog/2023/may/folding-at-home\n\nI wonder how OpenMPI in R works… http://webhome.auburn.edu/~zengpen/tutorials/openMPI_with_R.pdf\nNumerical estimation of the heat equation…",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week6/week6.html#footnotes",
    "href": "week6/week6.html#footnotes",
    "title": "Week 6",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Clique_problem↩︎\nhttps://en.wikipedia.org/wiki/Branch_and_bound↩︎\nhttps://en.wikipedia.org/wiki/Graph_partition↩︎\nhttps://en.wikipedia.org/wiki/Partition_problem↩︎\nhttps://en.wikipedia.org/wiki/Vertex_cover↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week7/week7.html",
    "href": "week7/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Recap from Last Time\n\n\n\n\n\n\nBranch and Bound for solving Maximum Clique\n\n\n\n\nSplit the problem into two cases where one gives you a maximum possible size and the lower bound.\nPick a vertex \\(v\\). Take out the subgraph \\(G_v\\) of all nodes connected to \\(v\\) containing and also produce \\(G' = G \\ G_v\\).\nIn the branch containing \\(v\\), we assume \\(v\\) is in the maximal clique.\nIn the branch with \\(G'\\), we assume \\(v\\) is not in the clique.\nFor the upper bound, calculate the chromatic number or Lovász number\nFor the lower bound, run a fast clique finder heuristic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUBO for Maximum Clique\n\n\n\nWhy did we talk about the equivalent unconstrained minimization (QUBO):\n\\[H = -\\sum_{i=1}^n x_i + 2 \\sum_{(i,j) \\in \\bar E} x_i x_j\\]\n\n\n\n\n\n\n\n\n\nFor the graph above:\n\\[ \\begin{aligned}\nH = & -x_1 - x_2 - x_3 - x_4 - x_5 - x_6  \\\\\n& + 2x_1 x_3 + 2x_1 x_4 + 2x_1 x_6 \\\\\n& + 2x_2 x_4 + 2x_2 x_6 \\\\\n& + 2x_3 x_5 + 2x_3 x_6 \\\\\n& + 2x_5 x_6\n\\end{aligned}\n\\]\n\n\nFormulate an objective \\(f = \\text{objective} + \\text{penalty}\\).\nA classic example of this formulation is LASSO: \\(X \\beta - y ||_2 +\n\\lambda ||\\beta||_1\\).\nIt should never be favorable to break the constraints of the problem to achieve a more favorable of the objective function.\nThe graph partitioning problem: We want to get two as-equal-in-size as possible graphs by cutting the graph making the least possible edge-cuts.\nHow to design a QUBO for the graph-partition problem?\n\nInput graph \\(G = (V,E)\\) with \\(n\\) vertices\nIsing indicator \\(s_i \\in \\{ -1, +1 \\}\\) for left/right partitions.\nIsing Hamiltonian \\(H = H_A + H_B\\) where the objective function for edge cut is \\[H_A = A \\sum_{(u,v) \\in E} \\frac{1-s_us_v}{2}.\\] This formula is the edge-cut.\nPenalty term for partition imbalance (penalty of \\(B\\) for each edge connecting vertices belonging to different patritions): \\[H_B = B\\left( \\sum_{i=1}^n s_i \\right)^2.\\]\n\nSee https://en.wikipedia.org/wiki/Graph_partition.\n\n\n\n\n\n\nGeorg’s favorite sentence of all time:\n\n\n\nChoose \\(A\\) and \\(B\\) so that it is never favorable to violate the penalty term to gain a further reduction in the objective function.\n\n\nContinuing, how do we pick \\(A\\) and \\(B\\)?\n\nWe now have the Ising Hamiltonian with \\(H = H_A + H_B\\) with \\[H_A = A \\sum_{(u,v) \\in E} \\frac{1-s_us_v}{2} \\quad \\text{ and }\n  H_B = B\\left( \\sum_{i=1}^n s_i \\right)^2.\\]\n\nWe want to take the best case scenario and the worst case scenario to make sure the penalty term always prevents\n\nAssume \\(s = (s_1,..., s_n) \\in \\{ -1, +1 \\}^n\\) is a global minimum. Let \\(d\\) be the degree of the graph (maximal number of edges for any vertex). When fliping one \\(s_v\\) for some \\(v \\in V\\), we gain a decrease of \\(|\\Delta H_A| \\leq d \\cdot A\\) in the best case. The best case occurs if \\(v\\) was in a different partition than all its neighbors.\nWe have seen that \\(|\\Delta H_A| \\leq d \\cdot A\\).\nThe penalty satisfies \\(\\Delta H_B \\geq 4 B\\) since \\(s_v\\) was compensated in equilibrium (the optimal solution) by another variable in the \\(H_B\\) term (if two spins \\(s_1\\) and \\(s_2\\) cancel out each other in \\(H_B\\), changing one of them yields \\(s_1 + s_2 \\in \\{ -2, +2\\}\\), thus the squaring in \\(H_B\\) gives a factor of 4).\nWe have seen that \\(|\\Delta H_A| \\leq d \\cdot A\\) and \\(\\Delta H_B \\geq 4B\\).\nTo discourage a violation of the penalty to gain a further reduction in the objective function, we need to enforce that \\(\\Delta H_B &gt; \\Delta H_A\\). This surely holds true if\n\n$\\(\\Delta H_B \\geq 4B \\stackrel{!}{&gt;} d \\cdot A \\geq |\\Delta H_A|\\).\nThus \\(B &gt; d \\cdot A / 4.\\)\n\nThe choice \\(A = 1\\) and \\(B = d/4 + 1\\) works.\n\nNotice that if \\(x_i \\in \\{ 0, 1\\}\\), then \\(x_i = x_i^2\\). So \\[Q = \\sum a_{ii} x_i^2 + \\sum_{i&lt;j} a_{ij} x_i x_j\\] has the form \\[(x_1, ..., x_n) \\begin{pmatrix}\na_{11} & a_{12} & \\cdots \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\cdots & \\cdots & \\ddots\n\\end{pmatrix} \\begin{pmatrix}\nx_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} = x^T Q x.\\]\n\n\n\n\n\n\nLook up the roof dual lower bound of the quadratic unconstrained lower bound.\nGeneralized roof duality\nQuantum Bridge Analytics I: A Tutorial on Formulating and Using QUBO Models\nLook at:\n\n2-satisfisability problem: wikipedia, geeksforgeeks\nQUBO: https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization\n(Lucas 2014)\n\n\n\n\n\n\n\nLucas, Andrew. 2014. “Ising Formulations of Many NP Problems.” Frontiers in Physics 2. https://doi.org/10.3389/fphy.2014.00005.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  }
]