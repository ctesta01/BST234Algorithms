[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "",
    "text": "Building 2, room 435\nResearch interests: algorithmic mathematics and statistics, efficient and randomized algorithms, methodology development, multiple testing, NP-complete problems.\nTA: Max Wang\nTake home midterm and final presentation project.\nWeek after of Spring Break is midterm."
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "2  Week 1: Intro",
    "section": "",
    "text": "What is an algorithm?\n\nA (finite) sequence of instructions that transforms some (well-specified) input into (well-specified) output.\n\nDefinition from wikipedia:\n\nIn mathematics and computer science, an algorithm is an effective method expressed as a finite list of well-defined instructions for calculating a function.\n\nAlgorithm = “A tool for solving a well-defined computational problem”\nAlgorithm \\(f:\\) input \\(\\to\\) output.\nProperties?\n\nRun-time (time complexity)\nSpace complexity (how much disk space is taken up)\n\nSorting can be done in place\n\nDecideability\n\nWhen a computer gets to a step, it has to know what to do.\n\n\nOften time and space complexity play off each other.\nRhetorical appeal: just look on Instagram — we store so much stuff. Apparently space is cheap.\n1. do x \n   or do y \n\n# not an algorithm\n1. flip coin \n2. if heads -> x \n      tails -> y\n\n# is an algorithm\nRandomness comes in different flavors:\n\nQuicksort is a method that always sorts, but its run-time is random.\nMCMC uses randomness in the result, because if we run it twice the results are different.\n\nRuntime and correctness are the gold-standard in methodology development. Prove runtime, prove correctness in the p-sets.\nInput: a set of numbers \\(\\langle a_1, a_2, ..., a_n \\rangle\\).\nOutput: Sorted sequence of input data \\(\\langle a_1', a_2', ..., a_n' \\rangle\\) with \\(a_1' \\leq ... \\leq a_n'\\).\nA sorting algorithm \\(f\\) solves the problem defined by the input-output relationship.\nThere are many sorting techniques."
  },
  {
    "objectID": "index.html#goals-for-the-course",
    "href": "index.html#goals-for-the-course",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "Goals for the course",
    "text": "Goals for the course\nIntroduction to important computational problems in computer science (biostatistics) and state of the art algorithms for solving them.\nUnderstanding of data structures and algorithms to solve problems of practical relevance.\nUnderstanding of mathematical standards for numerical analysis and statistics, inclusding their implementations.\nKnowledge of R and Python is vital (lab sessions).\nWhy is it always “Data Structures And Algorithms”?\nBecause the two are intrinsically linked; they don’t mean much without the other.\nWhy? Because the algorithm may run much faster or slower depending on the data structure.\nFor example, we might have a list. Or we might have a hash-map. Some things are fast in lists, whereas different things are fast in a hash-map.\nOften we have an end-pointer if we have a doubly-linked list.\nFor example, popping the first element, or last element, these are \\(O(1)\\) operations (including updating pointers). In contrast, searching is an \\(O(n)\\) operation when traversing.\nIf we have a binary tree (heap), then searching is \\(O(\\log_2 n )\\).\nWe will use the Cormen, Leiserson, Rivest, Stein. CLLS. Available for free on HOLLIS.\nContents\n\nIntroduction, random numbers\nConcepts of algorithms, complexity and sorting algorithms\nData structures and heapsort\nGreedy algorithms and dynamic programming\nIntroduction to parallel programming\nP and NP\nNumerical aspects of computer algorithms, condition of a problem, numerical stability of an algorithm, numerical error, forward/backward-error\nEfficient algorithms for linear algebra\nLeast-Squares Program, Eigenvalue Decomposition\nNumerical integration, Monte Carlo integration, importance sampling\nNumerical optimization\nWork on projects\nProject presentations"
  },
  {
    "objectID": "week1/week1.html#section",
    "href": "week1/week1.html#section",
    "title": "Week 1: Intro",
    "section": "",
    "text": "An iconic algorithm:\n\nfunction gcd(a,b): \n  while a ≠ b:\n    if a > b:\n      a := a-b\n    else: \n      b := b-a\n  return a\n\ngcd: 20, 15 -> 5 \n\na = 20, b = 15 \na = 5, b = 15, \na = 5, b = 1 \na = 5, b = 5\n\nThis algorithm is the greatest common divisor.\nThis is used in encryption and it is still the fastest algorithm.\nFrom ~300 BC from Euclid of Alexandria.\nExamples of algorithms:\n\nSorting Algorithms\nOptimization Problems:\n\nTraveling salesman: chip design, airline schedule\n\nMathematical problems:\n\nSolution of linear equations / matrix algebra\nIntegration\n\nStatistical problems\n\nOptimal designs / power calculations\nComputation of distributions\nComputation of test statistics\n\n\nExamples:\n\nEuclid GCD\nPagerank\nGradient descent\n\nLocal minimum\n\nBellman; Ford; shortest path and allow negative weights\nDjikstra\nCompression\nWord2Vec\nShannon Encoding for Transmission Errors\nTower of Hanoi\n\nRecursive\n\n\nGeorg’s hit-list:\n\nEuclid\nQuadratic Sieve\nBerlekamp\nCantor-Zassehaus\n\nFactoring numbers is super-polynomial but sub-exponential. It’s like almost exponential. This doesn’t go for factoring polynomials.\nWhen multiplying numbers, all the terms get jumbled up whereas we know where the pieces came from in polynomials:\n(123 * 827) = (1*10^2 + 2 * 10^2 + 3 * 10^0 ) * (...)\n\nvs. \n\n(2x + 3)(x^2 + 1)\n\nWhat do random numbers and compression have in common?\n\nProperties of Algorithms\n\nFeasibility\nTermination\nDeterministic\nFinite\n\nAlgorithms work on dynamic sets of elements (input -> output):\n\nSearching, inserting and deleting elements\nMaximum, minimum element\n\nData structures are used for the implementation of dynamic datasets. The efficiency of a data structure can depend on the desired manipulation operation.\nExamples of data structures:\n\nArray: access to elements based on index\n\nContinuous allocated memory, evenly divided\n\nLinked List: reference / pointer to the next element\nStack: dynamic set of elements, last in first out\nQueue: dynamic set of elements, can only be read starting from most longest added element, first-in-first-out\nGraphs or Trees: elements have references/pointers to a variable number of other elements\n\nCompare and contrast the array, doubly-linked list, and the heap:\n\n\n\n\n\n\n\nThe array has the property that if every element is of the same structure and takes up the same amount of memory, we can access the nth element quickly by calculating where it will be in memory and looking there. However, to update an array can be annoying because if it is growing and the neighboring memory is not free, we might have to move the whole array.\nThe doubly-linked list facilitates things like:\n\nEasy insertions or deletions anywhere\nQuickly accessing the first and last elements\nEasy storage in memory since memory doesn’t need to be in one contiguous block\n\nBut search algorithms are limited to stepping through the list until a desired element is found.\nOn the other hand, the heap allows for intelligent searching. Of course heaps aren’t good for everything: for example, accessing the nth element would be easier in an array than in a heap.\nReferences:\nBerlekamp’s algorithm https://en.wikipedia.org/wiki/Berlekamp%27s_algorithm\nRotating doubly-linked list: https://tex.stackexchange.com/questions/242044/add-label-to-a-rotated-doubly-linked-list"
  },
  {
    "objectID": "week1/week1.html#day-2",
    "href": "week1/week1.html#day-2",
    "title": "Week 1: Intro",
    "section": "Day 2",
    "text": "Day 2\nWe’ve moved classrooms to Kresge G1. On 7th of February, and 4th of March we need to be in FXB G12.\nLast time we talked about algorithms like sorting, searching, talking about their time / space complexity.\nAlgorithms need to have finite numbers of instructions.\nDeterministic means there is no randomness.\nWell-definedness means that the computer knows what to do at every step.\nProperties of algorithms include:\n\nFeasibility\nTermination\nDeterministic\n\nThe output is well-defined for every instance\nAt any time, the next step is well-defined\n\nFinite\n\nThe number of steps must be finite\nAt any time point, the required memory must be finite\n\n\nToday let’s discuss about random numbers.\nMotivation could include:\n\nStatistical simulation (Monte Carlo) in statistical methods research.\nThe statistical theories/methods are all based on assumptions. So most theorems state assumptions…\nThe theories can hardly be verified in real world data because\n\n\nthe real data never satisfy the assumption; and (2) the underlying truth is unknown (gold standard)\n\n\nIn simulation, data are created in a well controlled environment (model assumptions) and all truth are known. So the claim in the theorem can be validated and subjected to sensitivity analysis.\n\nWe often use the inverse CDF method to generate random numbers that are distributed a certain way. Or importance sampling. Or metropolis-hastings.\nBut how do we generate uniform random numbers? runif(10).\n\nCould it be just a list somewhere just stored\nIt could be some highly erratic but still deterministic function.\n\nWhy are ZIP CRC Checksums related to randomness? How do those relate to random numbers?\nWhatever we use to produce random numbers, we can perform statistical tests on the output to see if it’s distinguishable from randomness.\nSome good properties of a random number generator:\n\nUniformity\nIndependence\nDiehard tests (They should pass “Diehard” tests, qc-tests for PRNG)\nReplication\nCycle length - should be a long time before numbers repeat\nSpeed (fast)\nMemory usage (little need)\nParallel implementation (desirable for speed)\nCryptographically secure: required for password storage\n\nThe last point refers to the idea that there should be no ‘reversal’; it should not be possible to look at the random numbers generated and tell what the seed is.\nIf we have \\(f: s \\to [0,1]\\), we want to check two things:\n\nIs the output erratic? (Crucial)\nDoes $f^{-1} exist? (Can it be hacked)? (Optional)\n\n\nMid-square method\n\nStart with a 4-digit number \\(z_0\\), then square it. Should be an 8 digit number, and if not pad it.\nDeterministic sequence of numbers $e.g., \\(z_{n+1} = f(z_n)\\). Look at the middle four digits of it. Put the decimal in front.\nRepeat\n\nWe get uniform random number.\nBut this has problems, like two successive zeroes behind the decimal will never disappear.\n\n\nLinear Congruential Generator\nLCG is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. They produce a sequence of integers between 0 and m-1 according to \\(z_n = (a * z_{n-1} + c) \\pmod m, \\quad n = 1, 2, ...\\)\nwhere \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) the modulus. To obtain uniform random numbers on \\((0,1)\\), we take\n\\[u_n = z_n / m.\\]\nGood choices for \\(a\\), \\(c\\), and \\(m\\) are important.\nMarsaglia showed that \\((z_i, z_{i+1})\\) have this non-random structure.\nGood question: Is there a way to determine the number of hyperplanes?\nThe theory behind LCG is relatively easy to understand, and are easily implemented and fast, especially on computer hardware which can provide modulo arithmetic by stoage-bit truncation.\nA linear congruential generator has full period (=cycle length m) if and only if\n\nThe only positive integer that divides both \\(m\\) and \\(c\\) is 1 (the gcd)\nIf \\(q\\) is a prime number that divides \\(m\\), then \\(q\\) divides \\(a-1\\);\nIf 4 divides \\(m\\), then 4 divides \\(a-1\\).\n\nMarin Mersenne\nFor each Mersenne number, it’s prime if and only if it does not divide the corresponding shadow sequence number: \\(S_2 = 4\\), \\(S_n = S_{n-1}^2 - 2\\).\nimport numpy as np\nimport hashlib\nimport matplotlib.pyplot as plt\n\ndef hashing(s):\n    h = hashlib.md5()\n    #h = hashlib.sha256()\n    h.update(str(s).encode(\"utf-8\"))\n    return(h.hexdigest())\n\ndef random_number(seed):\n    temp = hashing(str(seed))\n    x = int(temp,16) / (16**len(temp))\n    return(x)\n\nif __name__==\"__main__\":\n    print(hashing(\"mouse\"))\n    print(hashing(\"house\"))\n    print(random_number(5))\n    # plot n numbers\n    n = 1000\n    x = np.arange(n)\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = random_number(x[i])\n    plt.plot(x,y,\"+\",color=\"black\")\n    plt.show()\nJohn the Ripper tries to invert the MD5 hashsum.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip"
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "In FXB G12 on Mondays, Kresge LL6 on Wednesdays."
  },
  {
    "objectID": "week2/week2.html#concepts-of-algorithms",
    "href": "week2/week2.html#concepts-of-algorithms",
    "title": "Week 2",
    "section": "Concepts of Algorithms",
    "text": "Concepts of Algorithms\nContents:\n\nConcepts of algorithms: what is an algorithm? What flavors of algorithms exist?\n\n\nComplexity in time and space\nRecursion\nDivide-and-conquer paradigm\nExample: Sorting algorithms\nExample: Traditional and fast multiplication via divide-and-conquer.\n\n\n\n\n\n\n\nNote\n\n\n\nWhat is an algorithm?\nA finite list of instructions that is well-defined. We can think of it as a function from input to output space: \\(f: S \\to T\\).\nWhat properties do they have?\n\nRuntime: They can terminate after finite time, or have finite expected runtime.\nComplexity in time & space.\nAlgorithms can be iterative, or recursive.\n\n\n\nAn instance of is a particular implementation of an algorithm. It should satisfy all the same properties as the original algorithm.\nThings to keep in mind:\n\nEvery step must be feasible.\nThe algorithm terminates after a finite number of steps.\nDeterminisms\n\n\n\n\n\n\n\nTip\n\n\n\nThere are two types of randomness: those that go into results, or those that don’t.\nTwo examples:\n\nDrawing random numbers; MCMC\nSorting numbers (random runtime)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s consider the runtime of insertion sort:\nIn step \\(i\\), we look at \\(n-i\\) values to compare.\n\\[\n\\sum_{i=1}^{n-1} n-i = \\sum_{i=1}^{n-1} i = \\frac{n(n-1)}{2} \\sim n^2\n\\]\n\\[\n\\sum_{i=1}^n i^2 \\sim n^3\n\\]\nIn general, is it always one more?\nYes, because we perform \\(n\\) operations of \\(i^{...}\\) complexity.\nWe can use comparison of an integral to a summation\n\\[\n\\sum_{i=1}^\\infty i^2 \\leq \\int_{i=1}^{\\infty} i^2\n\\]\nInsertion Sort(A)\n#| eval: false\nfor j <- 2 to length[A]\n  do key <- A[j]\n  > Insert A[j] into the sorted sequence A[1..j-1].\n  i <- j-1\n  while i > 0 and A[i] > key\n   do A[i+1]\n     i <- i - 1\n   A[i+1] <- key\n\n\n\\begin{algorithm} \\caption{Insertion-Sort} \\begin{algorithmic} \\Procedure{InsertionSort}{$A$} \\For{$j = p$ \\To $r - 1$} \\State $y \\gets A[j]$ \\State $\\triangleright$ Insert $A[j]$ into the sorted sequence $A[1..j-1]$ \\State $i \\gets j-1$ \\EndFor \\While{$i > 0$ and $A[i] > key$} \\State $A[i+1] \\gets A[i]$ \\State $i \\gets i - 1$ \\EndWhile \\State $A[i + 1] \\gets key$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nOne way of proving correctness of a sequential algorithm consists in finding an invariant which is always true in each iteration independently of the progress of the algorithm.\nHere: What is the invariant statement for insertion sort? The invariant is that in iteration i, in the array A[1..i-1] are sorted."
  },
  {
    "objectID": "week2/week2.html#growth-of-functions-and-mathcal-o-notation",
    "href": "week2/week2.html#growth-of-functions-and-mathcal-o-notation",
    "title": "Week 2",
    "section": "Growth of Functions and \\(\\mathcal O\\)-notation",
    "text": "Growth of Functions and \\(\\mathcal O\\)-notation\nWe define for functions \\(f(n)\\) and \\(g(n)\\):\n\nAsymptotically tight bound: \\[\n    \\begin{align*}\n    \\mathcal O(g(n)) & = \\{ f(n) :  \\text{ there exist constants } c_1, c_2 >\n    0 \\\\\n    & \\text{ and } n_0 \\text{ such that } 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\forall n \\geq n_0 \\}\n    \\end{align*}\n    \\]\nLitte-o notation (convergence to zero):\n\n\\[\\mathcal o(g(n)) = \\left\\{ f(n) \\colon \\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = 0\\right\\}\\]\n\n“Big-O” notation (asymptotic upper bound): \\[\n  \\mathcal O(g(n)) = \\left\\{\n    \\begin{align*}\n    & f(n) : \\text{there exists constants } c > 0 \\text{ and } \\\\\n    & n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n)\n    \\forall n \\geq n_0\n    \\end{align*}\n    \\right\\}\n  \\]\n\n\n\n\\begin{algorithm} \\caption{If-Example} \\begin{algorithmic} \\Procedure{If-Example}{$x$} \\If{$x < 100$} \\State $y \\gets x$ \\Else \\For{$i \\gets 1$ \\To $n$} \\If{$A[i] > y$} \\State $y \\gets A[i]$ \\EndIf \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nUpper-triangulization (Gaussian elimination) is \\(\\mathcal O(n^3)\\).\nFor calculating the determinant, we can use either:\n\nLaplacian expansion: (use minors)\n\nWhich gives us \\(\\mathcal O(n!) \\subset \\mathcal O(e^n)\\).\n\nDo Gaussian elimination and then multiply the diagonal: \\(\\mathcal O(n^3)\\). So this is \\(\\mathcal O(n^3) + \\mathcal O(n)\\), which is just \\(\\mathcal O(n^3)\\)."
  },
  {
    "objectID": "week2/week2.html#wednesday",
    "href": "week2/week2.html#wednesday",
    "title": "Week 2",
    "section": "Wednesday",
    "text": "Wednesday\nP Set principles: Practice \\(\\mathcal O\\) notation, mathematical relations, induction, and a simple algorithm to analyze.\nLast time, we did insertion sort.\n\nRecursion\nA recursive algorithm is an algorithm which calls itself with ”smaller (or simpler)” input values.\nA sequential algorithm example:\n\n\n\n\n\\begin{algorithm} \\caption{Sequential-Example} \\begin{algorithmic} \\Procedure{Sequential-Example}{} \\State{res $\\gets$ 1} \\For{$i = 1$ \\To $n$} \\State res = res $\\cdot \\; i$ \\EndFor \\State \\textbf{return} res; \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Factorial} \\begin{algorithmic} \\Procedure{factorial}{n} \\If{$n = 1$} \\Return 1 \\Else \\Return {$n \\; \\cdot \\;$ \\textsc{factorial} $(n-1)$} \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\nAn example of a problem not amenable to divide-and-conquer is the traveling salesman problem. If we calculate the optimal route over half the problem, it’s not known how to combine sub-solutions.\nhttps://webgol.dinfo.unifi.it/OptimizationModels/TravelingSalesPerson.html\nTo use divide-and-conquer we have to be in a situation where we can\n\nDivide\nConquer\nCombine\n\n\n\nMerge-Sort\nThe crux is that we can combine two sorted lists in linear time.\nJust combining is \\(\\mathcal O(n)\\).\n\n\n\\begin{algorithm} \\caption{Merge} \\begin{algorithmic} \\Procedure{Merge}{$a_1, a_2$} \\State $i \\gets 1$ \\State $j \\gets 1$ \\State $k \\gets 1$ \\State $c \\gets $ \\textsc{Array}($n$) \\While{$i,j < n$} \\If{$a_{1i} \\leq a_{2j}$} \\State $c_k \\gets a_{1i}$ \\State $i \\gets i +1$ \\Else \\State $c_k \\gets a_{2j}$ \\State $j \\gets j + 1$ \\EndIf \\State $k \\gets k + 1$ \\EndWhile \\Return $c$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\\begin{algorithm} \\caption{MergeSort} \\begin{algorithmic} \\Procedure{MergeSort}{$a$} \\State $A_1 \\gets$ \\textsc{HalfOf}$(A)$ \\State $A_2 \\gets$ \\textsc{TheOtherHalf}$(A)$ \\State $S_1 \\gets$ \\textsc{MergeSort}$(A_1)$ \\State $S_2 \\gets$ \\textsc{MergeSort}$(A_2)$ \\Return \\textsc{Merge}$(S_1, S_2)$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\\usetikzlibrary{decorations, calc, arrows, arrows.meta, positioning}\n\n\\begin{tikzpicture}[\n  scale=4,\n  array/.style={rectangle, draw, inner sep=5pt, text=black, minimum width = 20pt, minimum height = 20pt}\n  ]\n\n  % Nodes --- \n  \\node[array] (n11) {38};\n  \\node[array, right=0cm of n11] (n12) {27};\n  \\node[array, right=0cm of n12] (n13) {43};\n  \\node[array, right=0cm of n13] (n14) {3};\n  \\node[array, right=0cm of n14] (n15) {9};\n  \\node[array, right=0cm of n15] (n16) {82};\n  \\node[array, right=0cm of n16] (n17) {10};\n\n  \\node[array, below left=.5cm of n11] (n21) {38};\n  \\node[array, right=0cm of n21] (n22) {27};\n  \\node[array, right=0cm of n22] (n23) {43};\n  \\node[array, right=0cm of n23] (n24) {3};\n\n  \\node[array, right=2cm of n24] (n25) {9};\n  \\node[array, right=0cm of n25] (n26) {82};\n  \\node[array, right=0cm of n26] (n27) {10};\n\n\\end{tikzpicture}\n\n\n\n\n\n\n1. Divide A into A_1, A_2\n2. S_1 = MergeSort(A_1)\n   S_2 = MergeSort(A_2)\n3. S = MergeSort(S_1, S_2)\nThe complexity of \\(\\texttt{MergeSort}\\) on \\(n\\) variables is \\(\\mathcal O(2 T(\\frac{n}{2}) + n)\\) where \\(T(n/2)\\) is the time complexity of \\(\\texttt{MergeSort}\\) on \\(n/2\\) variables.\nSo \\(\\mathcal O(\\texttt{MergeSort}) = 2 [ 2 T(n/4) + n/2] + n\\)…\n\\[ = 2^2 T(n / 2^2) + 2n\\]\n\\[ ... \\]\n\\[ = 2^{\\log_2{n}} \\cdot T(1) + \\log_2(n) n\\]\n\\[ T(n) = n + n \\cdot \\log_2(n) \\in \\mathcal O(n \\log_2(n))\\]\nFor divide-and-conquer problems, we have time complexity given by\n\nhow many sub-problems we divide into\nsize of sub-problems\neffort to marshall the sub-problems (combine)\n\n\\[T(n) = a \\cdot T(\\frac{n}{b}) + n^c.\\]\nNow we prefer to have \\(a\\) small, \\(b\\) big, and \\(c\\) small.\nThe Master theorem yields asymptotically tight bounds to recurrences from divide and conquer algorithms that partition an input into smaller subproblems of equal sizes.\nThe crucial element is thus: what is the (log) ratio of the number of new subproblems to the size of new subproblems. I.e., \\(\\log_b(a)\\), the critical exponent.\nTheorem (Master Theorem). Let the runtime of an algorithm be given as\n\\[T(n) = a T(\\frac{b}{n}) + f(n)\\]\nand let the critical exponent be\n\\[c_{\\text{crit}} = \\log_b(a) = \\log(\\text{n of subproblems})/\\log(\\text{subproblem size}).\\]\nThen we have three cases:\n\nIf \\(f(n) = \\mathcal O(n^c)\\) for some \\(c < c_{\\text{crit}}\\) then \\(T(n) = \\Theta(n^{c_{\\text{crit}}})\\).\nIf \\(f(n) = \\Theta(n^{c_{\\text{crit}}} \\log^k n)\\) then \\(T(n) = \\Theta(n^{c_{\\text{crit}}} \\log^{k+1} n).\\)\nIf \\(f(n) = \\Omega(n^c)\\) for some \\(c > c_{\\text{crit}}\\) then no definite statement can be made yet. However, if \\(a \\cdot f(n/b) \\leq k \\cdot f(n)\\) for some \\(k < 1\\) and large enough \\(n\\) then \\(T(n) = \\Theta(f(n)).\\)\n\n1 corresponds to work splitting and recombining a problem being dominated by work on subproblems.\n2 corresponds to work to split/recombin a problem being comparable to work on subproblems.\n3 is whre work to split/recombine a problem dominates subproblems.\nThe runtime of MergeSort can be expressed as \\[T(n) = 2 T\\left(\\frac{n}{2}\\right) + n\\]\nTherefore \\(a = b = 2\\) and \\(f(n) = n\\).\nThe critical exponent is \\(c_{\\text{crit}} = \\log_b(a) = 1\\).\nCase 2 applies since \\(f(n) = \\Omega(n)\\) with \\(k = 0\\).\nTherefore the runtime is\n\\[T(n) = \\Omega(n^{c_{\\text{crit}}} \\log^{k+1}n) = \\Theta (n \\log n).\\]\n\n\n\n\n\n\nWarning\n\n\n\nRemember that \\[n \\log_3(n) = \\frac{\\log_2(n)}{\\log_2(3)},\\] so it doesn’t matter which base of the \\(\\log\\) we use in \\(\\mathcal O\\) notation.\n\n\n\n\nMultiplication\nInstead of writing 3456 * 8957 in our standard long-form multiplication, we can use divide-and-conquer:\n\\[=(34*89*10^4) + (34*57*10^2) + (56*89*10^2) + 56*57\\]\n\\[T(n) = 4 \\cdot T(\\frac{n}{2}) + n\\]\n3456 * 8957\n34|56 * 89|57\n\n= (34 * 10^2 + 56) * (89 * 10^2 + 57)\n\n(34 + 56)(89 + 57)\n= 34*89 + 34*57 + 56*89 + 56*57\n\nT(n) = 3 T(n/2) + n\nc_{\\text{crit}} = \\log_2(3) = 1.6\nO(n^{1.6})"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "import numpy as np \n\nx = [1,2,3,4]\n\ndef merge(x,y):\n  i = 0\n  j = 0\n  z = []\n  while (i < len(x) and j < len(y)):\n    if (i == len(x)):\n      z.append(y[j])\n      j = j + 1\n      print(z)\n    elif (j == len(y)):\n      z.append(x[i])\n      i = i + 1 \n    elif (x[i] < y[j]):\n      z.append(x[i])\n      i = i + 1\n    else:\n      z.append(y[j])\n      j = j + 1\n  return z\ndef merge2(x, y):\n  px = 0\n  py = 0\n  z = np.zeros(x.size + y.size, dtype=int)\n\n  for i in range(len(z)):\n    if px<len(x) and py<len(y):\n      if x[px] <= y[py]:\n        z[i] = x[px]\n        px = px+1\n      else:\n        z[i] = y[py]\n        py = py+1 \n    elif px==len(x):\n      z[i] = y[py]\n      py = py+1\n    elif py==len(y):\n      z[i] = x[px]\n      px = px+1\n  return(z)\n\ndef mergesort(x):\n  if len(x) == 1:\n    return(x)\n  else:\n    mid = len(x)//2\n    s1 = mergesort(x[:mid])\n    s2 = mergesort(x[mid:])\n    return(merge2(s1, s2))\n\nn = 10\nx = np.random.randint(100,size=n)      \ny = np.random.randint(100,size=n)      \nx = np.sort(x)\ny = np.sort(y)\nmerge2(x,y)\n\nmergesort(x)\nExample cpp code for merge sort\n#include <iostream>\n#include <vector>\n#include <algorithm>\nusing namespace std;\n\nvector<int> merge(vector<int> x, vector<int> y){\n  int i = 0;\n  int j = 0;\n  vector<int> z;\n  while (i < x.size() && j < y.size()){\n    if (x[i] < y[j]){\n      z.push_back(x[i]);\n      i++;\n    }\n    else{\n      z.push_back(y[j]);\n      j++;\n    }\n  }\n  while (i < x.size()){\n    z.push_back(x[i]);\n    i++;\n  }\n  while (j < y.size()){\n    z.push_back(y[j]);\n    j++;\n  }\n  return z;\n}\nDemo of how to run code in Rcpp\n\nlibrary(Rcpp)\ncppFunction('\n  std::vector<int> merge(std::vector<int> x, std::vector<int> y){\n    int i = 0;\n    int j = 0;\n    std::vector<int> z;\n    while (i < x.size() && j < y.size()){\n      if (x[i] < y[j]){\n        z.push_back(x[i]);\n        i++;\n      }\n      else{\n        z.push_back(y[j]);\n        j++;\n      }\n    }\n    while (i < x.size()){\n      z.push_back(x[i]);\n      i++;\n    }\n    while (j < y.size()){\n      z.push_back(y[j]);\n      j++;\n    }\n    return z;\n  }\n')\nx <- c(1,2,3,4)\ny <- c(2,3,4,5)\nmerge(x,y)\n\n[1] 1 2 2 3 3 4 4 5\n\n\nExamples of the data structures we’re interested in include:\n\nArray\nLinked List\nStack (LIFO)\nQueue (FIFO)\nGraphs or Trees\n\n// struct link\n// {\n//   link* next;\n//   int data;\n// };\n\nstruct link\n{\n  link* next;\n  int data;\n  link(int a_data, link* a_next = 0): next(a_next), data(a_data) {}\n};\n\nlink* small_primes = new link(2, new link(3, new link(5, new link(7))));\n\n// or \n// \ntemplate<typename T> struct link\n{\n  link* next;\n  T data;\n  link(T a_data, link* a_next = 0): next(a_next), data(a_data) {}\n};\nimport numpy as np \n\nclass Node: \n  def __init__(self, data=None): \n    self.value = data\n    self.next = None\n\nx = np.random.randint(100,size=10)\nprint(x)\n\nstart = Node(data = x[0])\n\nprev = start\nfor i in range(1, x.size):\n  temp = Node(x[i])\n  prev.next = temp\n  prev = temp\n\nprint(start.value)\nprint(start.next.value)\n\nwhile True:\n  print(start.value)\n  if start.next == None:\n    break\n  start = start.next\nimport numpy as np \n\nclass Node: \n  def __init__(self, data=None): \n    self.value = data\n    self.left = None\n    self.right = None\n\ndef valsToTree(l):\n  newnode = Node()\n  newnode.value = l[0]\n  l = l[1:]\n  if len(l) == 1:\n    newnode.left = Node(l)\n  elif len(l) > 1:\n    mid = len(l)//2\n    newnode.left = valsToTree(l[:mid])\n    newnode.right = valsToTree(l[mid:])\n  return newnode"
  },
  {
    "objectID": "week2/week2.html#at-home",
    "href": "week2/week2.html#at-home",
    "title": "Week 2",
    "section": "At Home",
    "text": "At Home\n\nhttps://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)\nhttps://en.wikipedia.org/wiki/Big_O_notation\nhttps://en.wikipedia.org/wiki/Multiplication_algorithm#Long_multiplication\nhttps://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture3.pdf\nhttps://www.geeksforgeeks.org/advanced-master-theorem-for-divide-and-conquer-recurrences/\nhttps://www.tutorialspoint.com/design_and_analysis_of_algorithms/design_and_analysis_of_algorithms_masters_theorem.htm"
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Heaps\nA heap is a binary tree with the following properties:\nC++ code for inserting into a heap\nElement \\(A[1]\\) contains the root of the heap. The parent of node \\(i\\) is always \\(A[\\lfloor i/2 \\rfloor]\\). The left child of node \\(i\\) is \\(A[2i]\\) and the right child is \\(A[2i+1]\\).\nThe max-heap property is that \\(A[\\text{parent}(i)] \\geq A[i]\\) for all \\(i\\).\nPython class for Heap with a maxHeapify method:\nSince in the worst case, the problem reduces to size at least 2/3, we find that \\(T(n) = T(2/3 \\cdot n) + \\Theta(1)\\).\nSolving this with the Master Theorem gives \\(O(\\log n)\\).\nTo build a heap, we want to call MaxHeapify on all nodes. We can write this as Build-Max-Heap(A). We want to make sure to do this from the bottom-up. We find that Build-Max-Heap(A) runs in \\(O(n \\log n)\\) time.\nRecall the idea of divide-and-conquer algorithms: we divide the problem into self-similar problems that we can ideally solve fast and recombine quickly.\nGreedy methods work really well when we have the greedy choice property and optimal substructure.\n*Greedy choice property**: Prove that whenever there is a choice, one of the optimal choices is the greedy one. This justifies choosing the greedy option. Show that all but one of the subproblems resulting from the greedy choice are empty.\nOptimal substructure: This property is satisfied if an optimal solution to the problem contains within it optimal solutions to its suproblems. Therefore an optimal solution can be constructed efficiently from optimal solutions of its subproblems.\nWhat is the invariant of the algorithm?\nInvariant: At the start of each iteration of the while loop, the amount of money remaining to be paid is the sum of the denominations in S."
  },
  {
    "objectID": "week4/week4.html#heaps---day-2",
    "href": "week4/week4.html#heaps---day-2",
    "title": "Week 4",
    "section": "Heaps - Day 2",
    "text": "Heaps - Day 2\n\n\\usetikzlibrary{shapes, trees} \n\n\\tikzset{\n  heap/.style={\n    every node/.style={circle,draw},\n    level 1/.style={sibling distance=30mm},\n    level 2/.style={sibling distance=15mm}\n    level 3/.style={sibling distance=5mm}\n  }\n}\n\n\\begin{tikzpicture}[\n    scale=2, \n    heap\n    ]\n\n  \\node {16}\n    child {node {14}\n      child {node {8}\n        child {node {2}}\n        child {node {4}}\n      }\n      child {node {7} \n        child {node {1}}\n      }\n    }\n    child [sibling angle=10] {node {10}\n      child {node {9}}\n      child {node {3}}\n    };\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\nComparison of heapsort and quicksort:\n\nHeapsort works in place\nA heap is a complete tree, where every node’s children must be greater than their parents.\nHeapsort: \\(O(n \\log n)\\)\n\nIn heapsort, we know where our biggest element is: at the top. So we prune it, and we can re-establish the heap property in log(n) time-complexity. Recall that to update, we pick the last element in the heap (bottom row, right-most), and we can maxHeapify once more."
  }
]