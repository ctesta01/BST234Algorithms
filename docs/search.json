[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "",
    "text": "Goals for the course\nIntroduction to important computational problems in computer science (biostatistics) and state of the art algorithms for solving them.\nUnderstanding of data structures and algorithms to solve problems of practical relevance.\nUnderstanding of mathematical standards for numerical analysis and statistics, inclusding their implementations.\nKnowledge of R and Python is vital (lab sessions).\nWhy is it always “Data Structures And Algorithms”?\nBecause the two are intrinsically linked; they don’t mean much without the other.\nWhy? Because the algorithm may run much faster or slower depending on the data structure.\nFor example, we might have a list. Or we might have a hash-map. Some things are fast in lists, whereas different things are fast in a hash-map.\nOften we have an end-pointer if we have a doubly-linked list.\nFor example, popping the first element, or last element, these are \\(O(1)\\) operations (including updating pointers). In contrast, searching is an \\(O(n)\\) operation when traversing.\nIf we have a binary tree (heap), then searching is \\(O(\\log_2 n )\\).\nWe will use the Cormen, Leiserson, Rivest, Stein. CLLS. Available for free on HOLLIS.\nContents",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures (BST 234) Notes</span>"
    ]
  },
  {
    "objectID": "index.html#goals-for-the-course",
    "href": "index.html#goals-for-the-course",
    "title": "Data Structures and Algorithms (BST 234) Notes",
    "section": "",
    "text": "Introduction, random numbers\nConcepts of algorithms, complexity and sorting algorithms\nData structures and heapsort\nGreedy algorithms and dynamic programming\nIntroduction to parallel programming\nP and NP\nNumerical aspects of computer algorithms, condition of a problem, numerical stability of an algorithm, numerical error, forward/backward-error\nEfficient algorithms for linear algebra\nLeast-Squares Program, Eigenvalue Decomposition\nNumerical integration, Monte Carlo integration, importance sampling\nNumerical optimization\nWork on projects\nProject presentations",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures (BST 234) Notes</span>"
    ]
  },
  {
    "objectID": "week7/week7.html",
    "href": "week7/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Recap from Last Time\nFormulate an objective \\(f = \\text{objective} + \\text{penalty}\\).\nA classic example of this formulation is LASSO: \\(X \\beta - y ||_2 +\n\\lambda ||\\beta||_1\\).\nIt should never be favorable to break the constraints of the problem to achieve a more favorable of the objective function.\nThe graph partitioning problem: We want to get two as-equal-in-size as possible graphs by cutting the graph making the least possible edge-cuts.\nHow to design a QUBO for the graph-partition problem?\nSee https://en.wikipedia.org/wiki/Graph_partition.\nContinuing, how do we pick \\(A\\) and \\(B\\)?\nWe want to take the best case scenario and the worst case scenario to make sure the penalty term always prevents\n$\\(\\Delta H_B \\geq 4B \\stackrel{!}{&gt;} d \\cdot A \\geq |\\Delta H_A|\\).\nThus \\(B &gt; d \\cdot A / 4.\\)\nNotice that if \\(x_i \\in \\{ 0, 1\\}\\), then \\(x_i = x_i^2\\). So \\[Q = \\sum a_{ii} x_i^2 + \\sum_{i&lt;j} a_{ij} x_i x_j\\] has the form \\[(x_1, ..., x_n) \\begin{pmatrix}\na_{11} & a_{12} & \\cdots \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\cdots & \\cdots & \\ddots\n\\end{pmatrix} \\begin{pmatrix}\nx_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} = x^T Q x.\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "week7/week7.html#numerical-aspects-of-algorithms",
    "href": "week7/week7.html#numerical-aspects-of-algorithms",
    "title": "Week 7",
    "section": "Numerical Aspects of Algorithms",
    "text": "Numerical Aspects of Algorithms\n\nBinary representation of floating point numbers\nPrecision and accuracy\nVector norms\nRelative and absolute numerical errors\nWell-posed and ill-posed numerical problems\nCondition number and well/ill-conditioned problems\nSources of numerical errors\nStability of linear equation systems.\n\nTypes of errors: - in the model - in the input data - round-off errors - approximation errors/numerical method - errors due to arithmetic, caused by \\(+\\) or \\(\\times\\) operators.\nFor example, observe that \\(14 = 2^3 + 2^2 + 2\\). On older computers, we used to have problems with wrap-around.\nIn this class, we may write \\(\\{ \\oplus, \\ominus, \\odot, \\oslash \\}\\) to denote the machine equivalents of the operations \\(\\{ +, -, \\cdot, / \\}\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "week7/week7.html#numerical-error-propagated-vs.-computational-error",
    "href": "week7/week7.html#numerical-error-propagated-vs.-computational-error",
    "title": "Week 7",
    "section": "Numerical Error: Propagated vs. Computational Error",
    "text": "Numerical Error: Propagated vs. Computational Error\nWe consider a function \\(f\\) and its approximation \\(\\tilde f\\).\nA value \\(x\\) and its approximation \\(\\tilde x\\).\nThen: \\(||f(x) - \\tilde f(\\tilde x)|| = ||\\underbrace{f(x) - f(\\tilde x)}_{\\text{propagated error}} + \\underbrace{f(\\tilde x) - \\tilde f(\\tilde x)}_{\\text{computational error}}||\\)\nTotal error := propagated error + computational error.\nThe propagated error is not affected by the algorithm.\nThe computational error is affected by the algorithm, and is capturing the idea that we’re not actually computing what we want to compute (e.g., \\(f\\) differs from \\(\\tilde f\\)).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  }
]